{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89786fa5-d99a-4785-b41e-4c5d0d60793b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b636a8-c67e-403a-a41f-0ddda54763fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "#from pl_bolts.optimizers.lars import LARS\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "   \n",
    "from lightly.data import LightlyDataset\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "from lightly.transforms import SimCLRViewTransform   ,SimSiamViewTransform\n",
    "from lightly.transforms.utils import IMAGENET_NORMALIZE\n",
    "from lightly.utils import scheduler\n",
    "from lightly.utils.benchmarking import BenchmarkModule \n",
    "from lightly.loss import (\n",
    "    BarlowTwinsLoss,\n",
    "    DCLLoss,\n",
    "    DCLWLoss,\n",
    "    DINOLoss,\n",
    "    MSNLoss,\n",
    "    NegativeCosineSimilarity,\n",
    "    NTXentLoss,\n",
    "    PMSNLoss,\n",
    "    SwaVLoss,\n",
    "    TiCoLoss,\n",
    "    VICRegLLoss,\n",
    "    VICRegLoss,\n",
    "    memory_bank,\n",
    ")\n",
    "\n",
    "from lightly.models import ResNetGenerator, modules, utils\n",
    "from lightly.models.modules import heads, masked_autoencoder\n",
    "from lightly.transforms import (\n",
    "    #BYOLTransform,\n",
    "    #BYOLView1Transform,\n",
    "    #BYOLView2Transform,\n",
    "    DINOTransform,\n",
    "    FastSiamTransform,\n",
    "    MAETransform,\n",
    "    MSNTransform,\n",
    "    SimCLRTransform,\n",
    "    SimSiamTransform,\n",
    "    SMoGTransform,\n",
    "    SwaVTransform,\n",
    "    VICRegLTransform,\n",
    "    VICRegTransform,\n",
    ")\n",
    "step_log=50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa45ec8-56ad-4420-ade7-9ffa146a6ece",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96a515d-bdf2-42d8-855e-8bb121c414fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_root_dir = os.path.join(os.getcwd(), \"tf-logs\")\n",
    "path_to_train = \"Cifar10/train/\"\n",
    "path_to_knntrain = \"Cifar10/train/\"\n",
    "path_to_test = \"Cifar10/test/\"\n",
    "log_dataset=\"cifar10\"\n",
    "max_epochs = 100\n",
    "classes = 10\n",
    "num_workers = 10\n",
    "in_size=32\n",
    "batch_size = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dc2657-d0c6-4787-bbbe-39bed9aab30e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63a233bc-920d-47c8-886c-e101d6adf8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/torchvision/transforms/transforms.py:890: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "knn_k = 200\n",
    "knn_t = 0.1\n",
    "distributed = False\n",
    "sync_batchnorm = False\n",
    "gather_distributed = False\n",
    "n_runs = 1  \n",
    "lr_factor = batch_size / 128   \n",
    "devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    " \n",
    "\n",
    "if distributed:\n",
    "    strategy = \"ddp\"\n",
    "    # reduce batch size for distributed training\n",
    "    batch_size = batch_size // devices\n",
    "else:\n",
    "    strategy = \"auto\"  # Set to \"auto\" if using PyTorch Lightning >= 2.0\n",
    "    # limit to single device if not using distributed training\n",
    "    devices = min(devices, 1)\n",
    "     \n",
    "simclr_transform = SimCLRTransform(\n",
    "    input_size=in_size,\n",
    "    cj_strength=0.5,\n",
    ")\n",
    "\n",
    "# Use SimCLR augmentations with larger image size for SimMIM\n",
    "simmim_transform = SimCLRTransform(input_size=224)\n",
    "\n",
    "# Use SimSiam augmentations\n",
    "simsiam_transform = SimSiamTransform(input_size=in_size)\n",
    "\n",
    "# Multi crop augmentation for FastSiam\n",
    "fast_siam_transform = FastSiamTransform(input_size=in_size)\n",
    "\n",
    "# Multi crop augmentation for SwAV\n",
    "swav_transform = SwaVTransform(\n",
    "    crop_sizes=(128, 64),\n",
    "    crop_counts=(2, 6),  # 2 crops @ 128x128px and 6 crops @ 64x64px\n",
    "    cj_strength=0.5,\n",
    ")\n",
    "\n",
    "# Multi crop augmentation for DINO, additionally, disable blur for cifar10\n",
    "dino_transform = DINOTransform(\n",
    "    global_crop_size=128,\n",
    "    local_crop_size=64,\n",
    "    cj_strength=0.5,\n",
    ")\n",
    "\n",
    "# Two crops for SMoG\n",
    "smog_transform = SMoGTransform(\n",
    "    crop_sizes=(128, 128),\n",
    "    crop_counts=(1, 1),\n",
    "    crop_min_scales=(0.2, 0.2),\n",
    "    crop_max_scales=(1.0, 1.0),\n",
    "    cj_strength=0.5,\n",
    ")\n",
    "\n",
    "# Single crop augmentation for MAE\n",
    "mae_transform = MAETransform()\n",
    "\n",
    "# Multi crop augmentation for MSN\n",
    "msn_transform = MSNTransform(\n",
    "    random_size=128,\n",
    "    focal_size=64,\n",
    "    cj_strength=1.0,  # Higher cj_strength works better for MSN on imagenette\n",
    ")\n",
    "\n",
    "vicreg_transform = VICRegTransform(\n",
    "    input_size=in_size,\n",
    "    cj_strength=0.5,\n",
    ")\n",
    "\n",
    "# Transform  passing geometrical transformation for VICRegL\n",
    "vicregl_transform = VICRegLTransform(\n",
    "    global_crop_size=128,\n",
    "    n_local_views=0,\n",
    "    global_grid_size=4,\n",
    "    cj_strength=0.5,\n",
    ")\n",
    "\n",
    "normalize_transform = torchvision.transforms.Normalize(\n",
    "    mean=IMAGENET_NORMALIZE[\"mean\"],\n",
    "    std=IMAGENET_NORMALIZE[\"std\"],\n",
    ")\n",
    "\n",
    "# No additional augmentations for the test set\n",
    "test_transforms = torchvision.transforms.Compose(\n",
    "    [\n",
    "        torchvision.transforms.Resize((in_size,in_size)),\n",
    "        #torchvision.transforms.CenterCrop(in_size),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        normalize_transform,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# we use test transformations for getting the feature for kNN on train data\n",
    "dataset_train_kNN = LightlyDataset(input_dir=path_to_knntrain, transform=test_transforms)\n",
    "\n",
    "dataset_test = LightlyDataset(input_dir=path_to_test, transform=test_transforms)\n",
    "\n",
    "from lightly.transforms.multi_view_transform import MultiViewTransform\n",
    "from lightly.transforms import SimCLRViewTransform   ,SimSiamViewTransform\n",
    "\n",
    "sv = SimCLRViewTransform(\n",
    "    input_size=in_size,\n",
    "    cj_strength=0.5,\n",
    ")\n",
    "st = SimSiamViewTransform(\n",
    "    input_size=in_size,\n",
    ")\n",
    "\n",
    "\n",
    "def create_dataset_train_ssl(model):\n",
    "    \"\"\"Helper method to apply the correct transform for ssl.\n",
    "\n",
    "    Args:\n",
    "        model:\n",
    "            Model class for which to select the transform.\n",
    "    \"\"\"\n",
    "    model_to_transform = {\n",
    "        BarlowTwinsModel: simclr_transform,\n",
    "        VICRegModel: MultiViewTransform([sv for _ in range(2)]),\n",
    " \n",
    "        VICRegModel_mean: MultiViewTransform([sv for _ in range(4)]),\n",
    "        BYOLModel: MultiViewTransform([sv for _ in range(2)]),#MultiViewTransform([st for _ in range(2)]),\n",
    "        BYOLModel_Mean: MultiViewTransform([sv for _ in range(4)]),\n",
    "        DCL: MultiViewTransform([sv for _ in range(2)]),\n",
    "        DCL_mean: MultiViewTransform([sv for _ in range(4)]),\n",
    "        DCLW: simclr_transform,\n",
    "        DINOModel: dino_transform,\n",
    "        FastSiamModel: fast_siam_transform,\n",
    "        \n",
    "        MocoModel: MultiViewTransform([sv for _ in range(2)]),\n",
    "        MocoModel_mean: MultiViewTransform([sv for _ in range(4)]),\n",
    "        MocoModel_mean_Shuffle:MultiViewTransform([sv for _ in range(4)]),\n",
    "        NNCLRModel: MultiViewTransform([sv for _ in range(2)]),\n",
    "        NNCLRModel_mean: MultiViewTransform([sv for _ in range(4)]),\n",
    "        SimCLRModel: MultiViewTransform([sv for _ in range(2)]),\n",
    "        SimCLRModel_mean:MultiViewTransform([sv for _ in range(4)]),\n",
    "        SimSiamModel: MultiViewTransform([st for _ in range(2)]),\n",
    "        SimSiamModel_mean: MultiViewTransform([st for _ in range(4)]),\n",
    "        SwaVModel: swav_transform,\n",
    "        #SwaVModel_mean: swav_transform,\n",
    "        TiCoModel:MultiViewTransform([sv for _ in range(2)]),\n",
    "        TiCoModel_mean:MultiViewTransform([sv for _ in range(4)]),\n",
    "        SMoGModel: smog_transform,\n",
    "    }\n",
    "    transform = model_to_transform[model]\n",
    "    return LightlyDataset(input_dir=path_to_train, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857059c6-d4a3-418f-ba43-f9a73a189c05",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ceaf1fa-0bc6-4ee8-b1ff-2e62a768783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(batch_size: int, dataset_train_ssl):\n",
    "    \"\"\"Helper method to create dataloaders for ssl, kNN train and kNN test.\n",
    "\n",
    "    Args:\n",
    "        batch_size: Desired batch size for all dataloaders.\n",
    "    \"\"\"\n",
    "    dataloader_train_ssl = torch.utils.data.DataLoader(\n",
    "        dataset_train_ssl,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_train_kNN = torch.utils.data.DataLoader(\n",
    "        dataset_train_kNN,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    dataloader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    return dataloader_train_ssl, dataloader_train_kNN, dataloader_test\n",
    "\n",
    "class TiCoModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.projection_head = heads.TiCoProjectionHead(512, 1024, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = TiCoLoss()\n",
    "        self.warmup_epochs = 40 if max_epochs >= 800 else 20\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_momentum(self, x):\n",
    "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        z = z.detach()\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        momentum = scheduler.cosine_schedule(self.current_epoch, max_epochs, 0.996, 1)\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=momentum)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=momentum\n",
    "        )\n",
    "        x0 = x0.to(self.device)\n",
    "        x1 = x1.to(self.device)\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward_momentum(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=0.3 * lr_factor,\n",
    "            weight_decay=1e-4,\n",
    "            momentum=0.9,\n",
    "        )\n",
    "        cosine_scheduler = scheduler.CosineWarmupScheduler(\n",
    "            optim, self.warmup_epochs, max_epochs\n",
    "        )\n",
    "        return [optim], [cosine_scheduler]\n",
    "\n",
    "\n",
    "class TiCoModel_mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.projection_head = heads.TiCoProjectionHead(512, 1024, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = TiCoLoss()\n",
    "        self.warmup_epochs = 40 if max_epochs >= 800 else 20\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        p = self.projection_head(y)\n",
    "        \n",
    "        y2 = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        z = z.detach()\n",
    "        return p,z\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        momentum = scheduler.cosine_schedule(self.current_epoch, max_epochs, 0.996, 1)\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=momentum)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=momentum\n",
    "        )\n",
    "\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for _,z  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for p,_   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        loss=0\n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=0.3 * lr_factor,\n",
    "            weight_decay=1e-4,\n",
    "            momentum=0.9,\n",
    "        )\n",
    "        cosine_scheduler = scheduler.CosineWarmupScheduler(\n",
    "            optim, self.warmup_epochs, max_epochs\n",
    "        )\n",
    "        return [optim], [cosine_scheduler]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MocoModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        num_splits = 0 if sync_batchnorm else 8\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = heads.MoCoProjectionHead(512, 512, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = NTXentLoss(\n",
    "            temperature=0.1,\n",
    "            memory_bank_size=4096,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        return self.projection_head(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "\n",
    "        # update momentum\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        utils.update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        def step(x0_, x1_):\n",
    "            x1_, shuffle = utils.batch_shuffle(x1_, distributed=distributed)\n",
    "            x0_ = self.backbone(x0_).flatten(start_dim=1)\n",
    "            x0_ = self.projection_head(x0_)\n",
    "\n",
    "            x1_ = self.backbone_momentum(x1_).flatten(start_dim=1)\n",
    "            x1_ = self.projection_head_momentum(x1_)\n",
    "            x1_ = utils.batch_unshuffle(x1_, shuffle, distributed=distributed)\n",
    "            return x0_, x1_\n",
    "\n",
    "        # We use a symmetric loss (model trains faster at little compute overhead)\n",
    "        # https://colab.research.google.com/github/facebookresearch/moco/blob/colab-notebook/colab/moco_cifar10_demo.ipynb\n",
    "        loss_1 = self.criterion(*step(x0, x1))\n",
    "        loss_2 = self.criterion(*step(x1, x0))\n",
    "\n",
    "        loss = 0.5 * (loss_1 + loss_2)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) + list(self.projection_head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "    \n",
    "class MocoModel_mean_Shuffle(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        num_splits = 0 if sync_batchnorm else 8\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = heads.MoCoProjectionHead(512, 512, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = NTXentLoss(\n",
    "            temperature=0.1,\n",
    "            memory_bank_size=4096,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x0_ = self.backbone(x).flatten(start_dim=1)\n",
    "        x0_ = self.projection_head(x0_)\n",
    "        \n",
    "        x1_, shuffle = utils.batch_shuffle(x, distributed=distributed)\n",
    "        x1_ = self.backbone_momentum(x1_).flatten(start_dim=1)\n",
    "        x1_ = self.projection_head_momentum(x1_).detach()\n",
    "        x1_ = utils.batch_unshuffle(x1_, shuffle, distributed=distributed)\n",
    "        return x0_,x1_\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        utils.update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        \n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for _,z  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for p,_   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        loss=0\n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) + list(\n",
    "            self.projection_head.parameters()\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "     \n",
    "    \n",
    "class MocoModel_mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        num_splits = 0 if sync_batchnorm else 8\n",
    "        resnet = ResNetGenerator(\"resnet-18\", num_splits=num_splits)\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # create a moco model based on ResNet\n",
    "        self.projection_head = heads.MoCoProjectionHead(512, 512, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # create our loss with the optional memory bank\n",
    "        self.criterion = NTXentLoss(\n",
    "            temperature=0.1,\n",
    "            memory_bank_size=4096,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    " \n",
    "        x0_ = self.backbone(x).flatten(start_dim=1)\n",
    "        x0_ = self.projection_head(x0_)\n",
    "\n",
    "        x1_ = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        x1_ = self.projection_head_momentum(x1_).detach()\n",
    " \n",
    "        return x0_,x1_\n",
    "\n",
    "         \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "        utils.update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        \n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for _,z  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for p,_   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        loss=0\n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.backbone.parameters()) + list(\n",
    "            self.projection_head.parameters()\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    " \n",
    "class SimCLRModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "    \n",
    "class SimCLRModel_mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z,z\n",
    " \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for _,z  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for p,_   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        loss=0\n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "class SimSiamModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.prediction_head = heads.SimSiamPredictionHead(2048, 512, 2048)\n",
    "        # use a 2-layer projection head for cifar10 as described in the paper\n",
    "        self.projection_head = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.ReLU(inplace=True)),\n",
    "                (2048, 2048, nn.BatchNorm1d(2048), None),\n",
    "            ]\n",
    "        )\n",
    "        self.criterion = NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(f)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach() \n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        \n",
    "        (x0, x1), _, _ = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        loss = 0.5 * (self.criterion(z0, p1) + self.criterion(z1, p0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2,  # no lr-scaling, results in better training stability\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class FastSiamModel(SimSiamModel):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]\n",
    "        zs = torch.stack([z for z, _ in features])\n",
    "        ps = torch.stack([p for _, p in features])\n",
    "\n",
    "        loss = 0.0\n",
    "        for i in range(len(views)):\n",
    "            mask = torch.arange(len(views), device=self.device) != i\n",
    "            loss += self.criterion(ps[i], torch.mean(zs[mask], dim=0)) / len(views)\n",
    "\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "class SimSiamModel_mean(SimSiamModel):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]\n",
    "        zs = torch.stack([z for z, _ in features])\n",
    "        ps = torch.stack([p for _, p in features])\n",
    "\n",
    "        loss=0\n",
    "        mean=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "class BarlowTwinsModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        # use a 2-layer projection head for cifar10 as described in the paper\n",
    "        self.projection_head = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.ReLU(inplace=True)),\n",
    "                (2048, 2048, None, None),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.criterion = BarlowTwinsLoss(gather_distributed=gather_distributed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    " \n",
    "\n",
    "class BYOLModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        # create a byol model based on ResNet\n",
    "        self.projection_head = heads.BYOLProjectionHead(512, 1024, 256)\n",
    "        self.prediction_head = heads.BYOLPredictionHead(256, 1024, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        return p\n",
    "\n",
    "    def forward_momentum(self, x):\n",
    "        y = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y)\n",
    "        z = z.detach()\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=0.99)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=0.99\n",
    "        )\n",
    "        (x0, x1), _, _ = batch\n",
    "        p0 = self.forward(x0)\n",
    "        z0 = self.forward_momentum(x0)\n",
    "        p1 = self.forward(x1)\n",
    "        z1 = self.forward_momentum(x1)\n",
    "        loss = 0.5 * (self.criterion(p0, z1) + self.criterion(p1, z0))\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "class BYOLModel_Mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        # create a byol model based on ResNet\n",
    "        self.projection_head = heads.BYOLProjectionHead(512, 1024, 256)\n",
    "        self.prediction_head = heads.BYOLPredictionHead(256, 1024, 256)\n",
    "\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        self.criterion = NegativeCosineSimilarity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        \n",
    "        y1 = self.backbone_momentum(x).flatten(start_dim=1)\n",
    "        z = self.projection_head_momentum(y1)\n",
    "        z = z.detach()\n",
    "        return p,z\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.backbone_momentum, m=0.99)\n",
    "        utils.update_momentum(\n",
    "            self.projection_head, self.projection_head_momentum, m=0.99\n",
    "        )\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for _,z  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for p,_   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        loss=0\n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "class SwaVModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "\n",
    "        self.projection_head = heads.SwaVProjectionHead(512, 512, 128)\n",
    "        self.prototypes = heads.SwaVPrototypes(128, 512)  # use 512 prototypes\n",
    "\n",
    "        self.criterion = SwaVLoss(sinkhorn_gather_distributed=gather_distributed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        x = self.projection_head(x)\n",
    "        x = nn.functional.normalize(x, dim=1, p=2)\n",
    "        return self.prototypes(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # normalize the prototypes so they are on the unit sphere\n",
    "        self.prototypes.normalize()\n",
    "\n",
    "        # the multi-crop dataloader returns a list of image crops where the\n",
    "        # first two items are the high resolution crops and the rest are low\n",
    "        # resolution crops\n",
    "        multi_crops, _, _ = batch\n",
    "        multi_crop_features = [self.forward(x) for x in multi_crops]\n",
    "\n",
    "        # split list of crop features into high and low resolution\n",
    "        high_resolution_features = multi_crop_features[:2]\n",
    "        low_resolution_features = multi_crop_features[2:]\n",
    "\n",
    "        # calculate the SwaV loss\n",
    "        loss = self.criterion(high_resolution_features, low_resolution_features)\n",
    "\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.Adam(\n",
    "            self.parameters(),\n",
    "            lr=1e-3 * lr_factor,\n",
    "            weight_decay=1e-6,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "    \n",
    "class SwaVModel_mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z,z\n",
    " \n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _, _ = batch\n",
    "        \n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for z,_  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for _,p   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "\n",
    "        # split list of crop features into high and low resolution\n",
    "        high_resolution_features_zs = zs[:2]\n",
    "        low_resolution_features_ps = ps[2:]\n",
    "        \n",
    "        loss=0\n",
    "        mean_embed_zs=torch.mean(high_resolution_features_zs, dim=0)\n",
    "        for i in range(len(low_resolution_features_ps)):\n",
    "            loss += self.criterion(low_resolution_features_ps[i], mean_embed_zs) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    " \n",
    "\n",
    "class NNCLRModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.prediction_head = heads.NNCLRPredictionHead(256, 4096, 256)\n",
    "        # use only a 2-layer projection head for cifar10\n",
    "        self.projection_head = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.ReLU(inplace=True)),\n",
    "                (2048, 256, nn.BatchNorm1d(256), None),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "        self.memory_bank = modules.NNMemoryBankModule(size=4096)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        z0 = self.memory_bank(z0, update=False)\n",
    "        z1 = self.memory_bank(z1, update=True)\n",
    "        loss = 0.5 * (self.criterion(z0, p1) + self.criterion(z1, p0))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    " \n",
    "\n",
    "class NNCLRModel_mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.prediction_head = heads.NNCLRPredictionHead(256, 4096, 256)\n",
    "        # use only a 2-layer projection head for cifar10\n",
    "        self.projection_head = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.ReLU(inplace=True)),\n",
    "                (2048, 256, nn.BatchNorm1d(256), None),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.criterion = NTXentLoss()\n",
    "        self.memory_bank = modules.NNMemoryBankModule(size=4096)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(y)\n",
    "        p = self.prediction_head(z)\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for z,_  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for _,p   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        \n",
    "        loss=0\n",
    "        \n",
    "        for i in range(len(views)):\n",
    "            if i==0:\n",
    "                zs[i] = self.memory_bank(zs[i], update=True)     \n",
    "            else:\n",
    "                zs[i] = self.memory_bank(zs[i], update=False)\n",
    "        \n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "    \n",
    "class DINOModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.head = self._build_projection_head()\n",
    "        self.teacher_backbone = copy.deepcopy(self.backbone)\n",
    "        self.teacher_head = self._build_projection_head()\n",
    "\n",
    "        utils.deactivate_requires_grad(self.teacher_backbone)\n",
    "        utils.deactivate_requires_grad(self.teacher_head)\n",
    "\n",
    "        self.criterion = DINOLoss(output_dim=2048)\n",
    "\n",
    "    def _build_projection_head(self):\n",
    "        head = heads.DINOProjectionHead(512, 2048, 256, 2048, batch_norm=True)\n",
    "        # use only 2 layers for cifar10\n",
    "        head.layers = heads.ProjectionHead(\n",
    "            [\n",
    "                (512, 2048, nn.BatchNorm1d(2048), nn.GELU()),\n",
    "                (2048, 256, None, None),\n",
    "            ]\n",
    "        ).layers\n",
    "        return head\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.head(y)\n",
    "        return z\n",
    "\n",
    "    def forward_teacher(self, x):\n",
    "        y = self.teacher_backbone(x).flatten(start_dim=1)\n",
    "        z = self.teacher_head(y)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        utils.update_momentum(self.backbone, self.teacher_backbone, m=0.99)\n",
    "        utils.update_momentum(self.head, self.teacher_head, m=0.99)\n",
    "        views, _, _ = batch\n",
    "        views = [view.to(self.device) for view in views]\n",
    "        global_views = views[:2]\n",
    "        teacher_out = [self.forward_teacher(view) for view in global_views]\n",
    "        student_out = [self.forward(view) for view in views]\n",
    "        loss = self.criterion(teacher_out, student_out, epoch=self.current_epoch)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        param = list(self.backbone.parameters()) + list(self.head.parameters())\n",
    "        optim = torch.optim.SGD(\n",
    "            param,\n",
    "            lr=6e-2 * lr_factor,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "class DCL(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = DCLLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "class DCL_mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = DCLLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z,z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for _,z  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for p,_   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        loss=0\n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "class DCLW(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        self.projection_head = heads.SimCLRProjectionHead(512, 512, 128)\n",
    "        self.criterion = DCLWLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1), _, _ = batch\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(\n",
    "            self.parameters(), lr=6e-2 * lr_factor, momentum=0.9, weight_decay=5e-4\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "class SMoGModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = ResNetGenerator(\"resnet-18\")\n",
    "        self.backbone = nn.Sequential(\n",
    "            *list(resnet.children())[:-1], nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "\n",
    "        # create a model based on ResNet\n",
    "        self.projection_head = heads.SMoGProjectionHead(512, 2048, 128)\n",
    "        self.prediction_head = heads.SMoGPredictionHead(128, 2048, 128)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "        # smog\n",
    "        self.n_groups = 300\n",
    "        memory_bank_size = 10000\n",
    "        self.memory_bank = memory_bank.MemoryBankModule(size=memory_bank_size)\n",
    "        # create our loss\n",
    "        group_features = torch.nn.functional.normalize(\n",
    "            torch.rand(self.n_groups, 128), dim=1\n",
    "        )\n",
    "        self.smog = heads.SMoGPrototypes(group_features=group_features, beta=0.99)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def _cluster_features(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        features = features.cpu().numpy()\n",
    "        kmeans = KMeans(self.n_groups).fit(features)\n",
    "        clustered = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "        clustered = torch.nn.functional.normalize(clustered, dim=1)\n",
    "        return clustered\n",
    "\n",
    "    def _reset_group_features(self):\n",
    "        # see https://arxiv.org/pdf/2207.06167.pdf Table 7b)\n",
    "        features = self.memory_bank.bank\n",
    "        group_features = self._cluster_features(features.t())\n",
    "        self.smog.set_group_features(group_features)\n",
    "\n",
    "    def _reset_momentum_weights(self):\n",
    "        # see https://arxiv.org/pdf/2207.06167.pdf Table 7b)\n",
    "        self.backbone_momentum = copy.deepcopy(self.backbone)\n",
    "        self.projection_head_momentum = copy.deepcopy(self.projection_head)\n",
    "        utils.deactivate_requires_grad(self.backbone_momentum)\n",
    "        utils.deactivate_requires_grad(self.projection_head_momentum)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if self.global_step > 0 and self.global_step % 300 == 0:\n",
    "            # reset group features and weights every 300 iterations\n",
    "            self._reset_group_features()\n",
    "            self._reset_momentum_weights()\n",
    "        else:\n",
    "            # update momentum\n",
    "            utils.update_momentum(self.backbone, self.backbone_momentum, 0.99)\n",
    "            utils.update_momentum(self.projection_head, self.projection_head_momentum, 0.99)\n",
    "\n",
    "        (x0, x1), _, _ = batch\n",
    "\n",
    "        if batch_idx % 2:\n",
    "            # swap batches every second iteration\n",
    "            x0, x1 = x1, x0\n",
    "\n",
    "        x0_features = self.backbone(x0).flatten(start_dim=1)\n",
    "        x0_encoded = self.projection_head(x0_features)\n",
    "        x0_predicted = self.prediction_head(x0_encoded)\n",
    "        x1_features = self.backbone_momentum(x1).flatten(start_dim=1)\n",
    "        x1_encoded = self.projection_head_momentum(x1_features)\n",
    "\n",
    "        # update group features and get group assignments\n",
    "        assignments = self.smog.assign_groups(x1_encoded)\n",
    "        group_features = self.smog.get_updated_group_features(x0_encoded)\n",
    "        logits = self.smog(x0_predicted, group_features, temperature=0.1)\n",
    "        self.smog.set_group_features(group_features)\n",
    "\n",
    "        loss = self.criterion(logits, assignments)\n",
    "\n",
    "        # use memory bank to periodically reset the group features with k-means\n",
    "        self.memory_bank(x0_encoded, update=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = (\n",
    "            list(self.backbone.parameters())\n",
    "            + list(self.projection_head.parameters())\n",
    "            + list(self.prediction_head.parameters())\n",
    "        )\n",
    "        optim = torch.optim.SGD(\n",
    "            params,\n",
    "            lr=0.01,\n",
    "            momentum=0.9,\n",
    "            weight_decay=1e-6,\n",
    "        )\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "\n",
    "# Note: The model and training settings do not follow the reference settings\n",
    "# from the paper. The settings are chosen such that the example can easily be\n",
    "# run on a small dataset with a single GPU.\n",
    "\n",
    "\n",
    "from lightly.loss.vicreg_loss import VICRegLoss\n",
    "from lightly.models.modules import BarlowTwinsProjectionHead\n",
    "from lightly.transforms.vicreg_transform import VICRegTransform\n",
    "\n",
    "class VICRegModel(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n",
    "        self.criterion = VICRegLoss(gather_distributed=gather_distributed)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z \n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        (x0, x1) = batch[0]\n",
    "        z0 = self.forward(x0)\n",
    "        z1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class VICRegModel_mean(BenchmarkModule):\n",
    "    def __init__(self, dataloader_kNN, num_classes):\n",
    "        super().__init__(dataloader_kNN, num_classes)\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        # create a ResNet backbone and remove the classification head\n",
    "        resnet = torchvision.models.resnet18()\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[0:2]+list(resnet.children())[4:-1], nn.AdaptiveAvgPool2d(1))  \n",
    "        self.projection_head = BarlowTwinsProjectionHead(512, 2048, 2048)\n",
    "        self.criterion = VICRegLoss(gather_distributed=gather_distributed)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x).flatten(start_dim=1)\n",
    "        z = self.projection_head(x)\n",
    "        return z,z\n",
    "\n",
    "    def training_step(self, batch, batch_index):\n",
    "        views, _, _ = batch\n",
    "        features = [self.forward(view) for view in views]   # features= [...,batch, feature for view_i]\n",
    "        zs = torch.stack([z for _,z  in features])     # zs= [...,embedding of batch, feature for view_i]\n",
    "        ps = torch.stack([p for p,_   in features])     # zs= [...,projection of batch, feature for view_i]\n",
    "        loss=0\n",
    "        mean_embed=torch.mean(zs, dim=0)\n",
    "        for i in range(len(views)):\n",
    "            loss += self.criterion(ps[i], mean_embed) / (len(views))    \n",
    "        self.log(\"train_loss_ssl\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, max_epochs)\n",
    "        return [optim], [scheduler]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e228a5-d76b-4fe2-93c8-7c791f301df2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689fae2-d753-45d6-8349-c7d1d1524a66",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "models = [\n",
    "     #MB DV TNS A \n",
    "\n",
    "     MocoModel_mean_Shuffle,\n",
    "     MocoModel,\n",
    "    \n",
    "     BYOLModel_Mean,\n",
    "     BYOLModel,\n",
    "    \n",
    "     DCL_mean,\n",
    "     DCL,\n",
    "    \n",
    "     VICRegModel_mean,\n",
    "     VICRegModel,\n",
    "    \n",
    "     TiCoModel_mean,\n",
    "     TiCoModel,\n",
    "     \n",
    "     NNCLRModel_mean,\n",
    "     NNCLRModel,\n",
    "    \n",
    "     SimCLRModel_mean,\n",
    "     SimCLRModel,\n",
    "    \n",
    "     SimSiamModel_mean,\n",
    "     SimSiamModel, \n",
    "]\n",
    "bench_results = dict()\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Determine if the file already exists\n",
    "file_exists = os.path.isfile('train_record.csv')\n",
    "\n",
    "experiment_version = None\n",
    "# loop through configurations and train models\n",
    "for BenchmarkModel in models:\n",
    "    runs = []\n",
    "    model_name = BenchmarkModel.__name__.replace(\"Model\", \"\")\n",
    "    for seed in range(n_runs):\n",
    "        pl.seed_everything(seed)\n",
    "        dataset_train_ssl = create_dataset_train_ssl(BenchmarkModel)\n",
    "        dataloader_train_ssl, dataloader_train_kNN, dataloader_test = get_data_loaders(\n",
    "            batch_size=batch_size, dataset_train_ssl=dataset_train_ssl\n",
    "        )\n",
    "        benchmark_model = BenchmarkModel(dataloader_train_kNN, classes)\n",
    "\n",
    "        # Save logs to: {CWD}/benchmark_logs/cifar10/{experiment_version}/{model_name}/\n",
    "        # If multiple runs are specified a subdirectory for each run is created.\n",
    "        sub_dir = model_name if n_runs <= 1 else f\"{model_name}/run{seed}\"\n",
    "        logger = TensorBoardLogger(\n",
    "            save_dir=os.path.join(logs_root_dir, log_dataset),\n",
    "            name=\"\",\n",
    "            sub_dir=sub_dir,\n",
    "            version=experiment_version,\n",
    "        )\n",
    "        if experiment_version is None:\n",
    "            # Save results of all models under same version directory\n",
    "            experiment_version = logger.version\n",
    "        checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=os.path.join(logger.log_dir, \"checkpoints\")\n",
    "        )\n",
    "        trainer = pl.Trainer(\n",
    "            max_epochs=max_epochs,\n",
    "            devices=devices,\n",
    "            accelerator=accelerator,\n",
    "            default_root_dir=logs_root_dir,\n",
    "            strategy=strategy,\n",
    "            sync_batchnorm=sync_batchnorm,\n",
    "            logger=logger,\n",
    "            callbacks=[checkpoint_callback],\n",
    "            check_val_every_n_epoch =5,\n",
    "            log_every_n_steps=step_log,\n",
    "        )\n",
    "        start = time.time()\n",
    "        trainer.fit(\n",
    "            benchmark_model,\n",
    "            train_dataloaders=dataloader_train_ssl,\n",
    "            val_dataloaders=dataloader_test,\n",
    "        )\n",
    "        end = time.time()\n",
    "        run = {\n",
    "            \"model\": model_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"epochs\": max_epochs,\n",
    "            \"max_accuracy\": benchmark_model.max_accuracy,\n",
    "            \"runtime\": end - start,\n",
    "            \"gpu_memory_usage\": torch.cuda.max_memory_allocated(),\n",
    "            \"seed\": seed,\n",
    "        }\n",
    "        file_exists = os.path.isfile('train_record.csv')\n",
    "        with open('train_record.csv', mode='a', newline='') as file:\n",
    "            csv_writer = csv.writer(file)\n",
    "            # If the file is newly created, write the header\n",
    "            if not file_exists:\n",
    "                csv_writer.writerow(['Model', 'GPU Memory Usage (GByte)', 'Runtime (Min)', 'Accuracy'])\n",
    "            csv_writer.writerow([run[\"model\"], run[\"gpu_memory_usage\"], run[\"runtime\"] / 60, run[\"max_accuracy\"]])\n",
    "        runs.append(run)\n",
    "        print(run)\n",
    "\n",
    "        # delete model and trainer + free up cuda memory\n",
    "        del benchmark_model\n",
    "        del trainer\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    bench_results[model_name] = runs\n",
    "\n",
    "# print results table\n",
    "header = (\n",
    "    f\"| {'Model':<13} | {'Batch Size':>10} | {'Epochs':>6} \"\n",
    "    f\"| {'KNN Test Accuracy':>18} | {'Time':>10} | {'Peak GPU Usage':>14} |\"\n",
    ")\n",
    "print(\"-\" * len(header))\n",
    "print(header)\n",
    "print(\"-\" * len(header))\n",
    "for model, results in bench_results.items():\n",
    "    runtime = np.array([result[\"runtime\"] for result in results])\n",
    "    runtime = runtime.mean() / 60  # convert to min\n",
    "    accuracy = np.array([result[\"max_accuracy\"] for result in results])\n",
    "    gpu_memory_usage = np.array([result[\"gpu_memory_usage\"] for result in results])\n",
    "    gpu_memory_usage = gpu_memory_usage.max() / (1024**3)  # convert to gbyte\n",
    "\n",
    "    if len(accuracy) > 1:\n",
    "        accuracy_msg = f\"{accuracy.mean():>8.3f} +- {accuracy.std():>4.3f}\"\n",
    "    else:\n",
    "        accuracy_msg = f\"{accuracy.mean():>18.3f}\"\n",
    "\n",
    "    print(\n",
    "        f\"| {model:<13} | {batch_size:>10} | {max_epochs:>6} \"\n",
    "        f\"| {accuracy_msg} | {runtime:>6.1f} Min \"\n",
    "        f\"| {gpu_memory_usage:>8.1f} GByte |\",\n",
    "        flush=True,\n",
    "    )\n",
    "print(\"-\" * len(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961f57b-fa0b-4e51-92c1-8449ad96753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f392bc4-81e7-4a0c-a965-d5ac653d3e02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
